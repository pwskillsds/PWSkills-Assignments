{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiKvQBRC0d0ZdPYsHMY5Pv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pwskillsds/PWSkills-Assignments/blob/main/Activation_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. An activation function in the context of artificial neural networks is a mathematical function applied to the weighted sum of the inputs of a neuron. It introduces non-linearity into the output of the neuron, allowing neural networks to model complex relationships\n",
        "between inputs and outputs.\n",
        "\n",
        "\n",
        "\n",
        "Q2. Some common types of activation functions used in neural networks are:\n",
        "\n",
        "Sigmoid function: It maps the input to a value between 0 and 1, which can be interpreted as a probability. It has a smooth, S-shaped curve.\n",
        "Rectified Linear Unit (ReLU): It returns the input directly if it is positive, otherwise, it returns zero. It is a piecewise linear function and is widely used in deep learning.\n",
        "Leaky ReLU: It is similar to ReLU but allows a small, non-zero gradient for negative inputs. It addresses the \"dying ReLU\" problem.\n",
        "Hyperbolic tangent (tanh): It maps the input to a value between -1 and 1. It is an S-shaped curve like the sigmoid but symmetric around the origin.\n",
        "Softmax: It takes a vector of real numbers as input and normalizes it into a probability distribution, with each element representing the probability of a class in a multi-class classification problem.\n",
        "\n",
        "\n",
        "\n",
        "Q3. Activation functions affect the training process and performance of a neural network in several ways:\n",
        "\n",
        "Non-linearity: Activation functions introduce non-linearities, allowing neural networks to model complex relationships between inputs and outputs. Without non-linear activation functions, a neural network would simply be a linear combination of its inputs.\n",
        "Gradient propagation: Activation functions influence how gradients flow backward through the network during the backpropagation algorithm, affecting the optimization process. The choice of activation function can impact the speed and stability of convergence during training.\n",
        "Output range: Different activation functions have different output ranges, which can affect the numerical stability of the network and the interpretation of the output values.\n",
        "Computational efficiency: Some activation functions are computationally more efficient to compute than others, which can impact the overall efficiency of training and inference in large-scale neural networks.\n",
        "\n",
        "\n",
        "Q4. The sigmoid activation function, also known as the logistic function, is defined as f(x) = 1 / (1 + e^(-x)). It squashes the input value to a range between 0 and 1, which can be interpreted as a probability. Its output values increase monotonically, and it has a smooth S-shaped curve.\n",
        "\n",
        "Advantages of the sigmoid function:\n",
        "\n",
        "It maps the input to a bounded range, which can be useful for certain applications like binary classification where a probability-like output is desired.\n",
        "It has a smooth derivative, which facilitates gradient-based optimization methods like backpropagation.\n",
        "Disadvantages of the sigmoid function:\n",
        "\n",
        "It suffers from the vanishing gradient problem, where gradients become very small for extreme input values, leading to slow convergence during training.\n",
        "It is not zero-centered, which can make optimization slower in certain cases.\n",
        "Its exponential calculations can be computationally expensive compared to other activation functions.\n",
        "\n",
        "\n",
        "\n",
        "Q5. The rectified linear unit (ReLU) activation function is defined as f(x) = max(0, x). It returns the input directly if it is positive, otherwise, it outputs zero. Unlike the sigmoid function, ReLU is a piecewise linear function and does not squash the input to a bounded range.\n",
        "\n",
        "The main difference between ReLU and the sigmoid function is that ReLU is non-linear only for positive inputs, whereas the sigmoid function is non-linear across its entire range. ReLU effectively \"turns off\" negative inputs, making it more sparse and computationally efficient compared to the sigmoid function.\n",
        "\n",
        "\n",
        "Q6. The benefits of using the ReLU activation function over the sigmoid function include:\n",
        "\n",
        "Sparse activation: ReLU sets negative inputs to zero, resulting in sparse activation where fewer neurons are activated. This can lead to more efficient computation and reduced overfitting.\n",
        "Avoiding vanishing gradients: ReLU does not suffer from the vanishing gradient problem to the same extent as the sigmoid function. It provides better gradient propagation, enabling faster convergence during training.\n",
        "Computational efficiency: ReLU is computationally efficient to compute compared to the sigmoid function, as it involves simpler mathematical operations.\n",
        "Biological plausibility: ReLU exhibits similar behavior to the firing of real neurons, making it more biologically plausible as an activation function.\n",
        "\n",
        "\n",
        "\n",
        "Q7. The concept of \"leaky ReLU\" is a variation of the ReLU activation function that addresses the vanishing gradient problem. In the standard ReLU, all negative inputs are set to zero. However, in leaky ReLU, a small slope or \"leak\" is introduced for negative inputs, allowing a small non-zero output.\n",
        "\n",
        "Mathematically, leaky ReLU is defined as f(x) = max(a * x, x), where a is a small positive constant (typically a small value like 0.01). When a is set to zero, leaky ReLU becomes the standard ReLU.\n",
        "\n",
        "By introducing a small slope for negative inputs, leaky ReLU prevents dead neurons (neurons that never activate due to always having negative inputs) and provides a non-zero gradient for negative inputs. This helps alleviate the vanishing gradient problem and can improve the training of deep neural networks.\n",
        "\n",
        "\n",
        "\n",
        "Q8. The softmax activation function is used primarily in multi-class classification problems. It takes a vector of real numbers as input and normalizes it into a probability distribution over multiple classes. It is commonly used in the output layer of a neural network for tasks like image classification, where the network needs to assign probabilities to each possible class.\n",
        "\n",
        "Mathematically, the softmax function computes the exponential of each input element, divides it by the sum of exponentials across all elements, and normalizes the resulting values to ensure they sum up to 1. This produces a probability distribution where each element represents the likelihood of the corresponding class.\n",
        "\n",
        "The softmax function is given by f(x_i) = e^(x_i) / sum(e^(x_j)) for each element x_i in the input vector.\n",
        "\n",
        "\n",
        "\n",
        "Q9. The hyperbolic tangent (tanh) activation function is similar to the sigmoid function but maps the input to a range between -1 and 1. It is defined as f(x) = (e^x - e^(-x)) / (e^x + e^(-x)).\n",
        "\n",
        "Compared to the sigmoid function, the tanh function is zero-centered, which can make optimization faster in certain cases. However, it still suffers from the vanishing gradient problem for extreme input values.\n",
        "\n",
        "Like the sigmoid function, the tanh function is also smooth and continuously differentiable, which allows for efficient gradient-based optimization. It is often used in hidden layers of neural networks, especially when inputs are standardized or normalized to have zero mean and unit variance.\n"
      ],
      "metadata": {
        "id": "cKy3DVO3zhqL"
      }
    }
  ]
}